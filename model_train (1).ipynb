{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "!pip install protobuf==3.20.2\n",
        "!pip install onnx==1.10.1\n",
        "!pip install onnxmltools==1.9.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "!pip install pyspark onnx scikit-learn skl2onnx onnxmltools pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# ... (your existing imports)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col,when\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF ,StringIndexer\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import split\n",
        "import onnxmltools\n",
        "from onnxmltools.convert.common.data_types import StringType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "spark = SparkSession.builder.appName(\"SentimentAnalysis\").master(\"local[*]\").config(\"spark.driver.maxResultSize\", \"8g\").config(\"spark.executor.memory\", \"6g\").getOrCreate()\n",
        "print(\"SparkSession created successfully\")\n",
        "\n",
        "test_file = \"hdfs://namenode:9000/user/input/test.ft.txt\"\n",
        "train_file = \"hdfs://namenode:9000/user/input/train.ft.txt\"\n",
        "\n",
        "test_df = spark.read.text(test_file)\n",
        "train_df = spark.read.text(train_file)\n",
        "\n",
        "print(\"\\nFirst few rows of test.ft.txt:\")\n",
        "#test_df.show(5, truncate=False)\n",
        "\n",
        "print(\"\\nFirst few rows of train.ft.txt:\")\n",
        "#train_df.show(5, truncate=False)\n",
        "print(\"---------------------------__--------------:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# drop nan\n",
        "test_df = test_df.na.drop()\n",
        "train_df = train_df.na.drop()\n",
        "\n",
        "test_df.count()\n",
        "train_df.count()\n",
        "\n",
        "# Train DataFrame\n",
        "split_col_tr = split(train_df[\"value\"], \" \", 2)\n",
        "train_df = train_df.withColumn(\"label\", when(split_col_tr.getItem(0) == \"__label__1\", 1.0).otherwise(2.0))\n",
        "train_df = train_df.withColumn(\"Comment\", split_col_tr.getItem(1))\n",
        "\n",
        "# Test DataFrame\n",
        "split_col_ts = split(test_df[\"value\"], \" \", 2)\n",
        "test_df = test_df.withColumn(\"label\", when(split_col_ts.getItem(0) == \"__label__1\", 1.0).otherwise(2.0))\n",
        "test_df = test_df.withColumn(\"Comment\", split_col_ts.getItem(1))\n",
        "\n",
        "# Use only the \"Comment\" column as the feature\n",
        "train_processed_df = train_df.select(\"Comment\", \"label\")\n",
        "test_processed_df = test_df.select(\"Comment\", \"label\")\n",
        "\n",
        "print(\"After preprocessing:\")\n",
        "train_processed_df.show(4, truncate=False)\n",
        "test_processed_df.show(4, truncate=False)\n",
        "\n",
        "train_processed_set, val_processed_set = train_processed_df.randomSplit([0.90, 0.10], seed=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "# Continue with the rest of your code for model training and evaluation...\n",
        "\n",
        "models = [\n",
        "    ('NaiveBayes', NaiveBayes()),\n",
        "    ('RandomForestClassifier', RandomForestClassifier())\n",
        "]\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for name, model in models:\n",
        "    tokenizer = Tokenizer(inputCol=\"Comment\", outputCol=\"words\")\n",
        "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
        "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "    indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_indexed\")\n",
        "    classifier = model.setLabelCol(\"label_indexed\").setFeaturesCol(\"features\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, indexer, classifier])\n",
        "\n",
        "    print(f\"Training {name} model...\")\n",
        "    model_fit = pipeline.fit(train_processed_set)\n",
        "    print(f\"{name} model trained successfully.\")\n",
        "\n",
        "    predictions = model_fit.transform(train_processed_set)\n",
        "    predictions_val = model_fit.transform(val_processed_set)\n",
        "\n",
        "    print(\"predictions-..........\")\n",
        "    #predictions.show(7, truncate=False)\n",
        "    print(\"predictions_val-..........\")\n",
        "    #predictions_val.show(7, truncate=False)\n",
        "\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    print(f'{name} Accuracy: {accuracy}')\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model_fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "import onnxmltools\n",
        "from onnxmltools.convert import convert_sparkml\n",
        "from onnxmltools.utils import save_model\n",
        "from onnxmltools.convert.common.data_types import StringType, FloatTensorType\n",
        "\n",
        "# Assuming best_model is your PipelineModel\n",
        "# Extract the final stage of the pipeline (presumably your NaiveBayesModel)\n",
        "final_model = best_model.stages[-1]\n",
        "print(final_model)\n",
        "# Define the input types based on your model\n",
        "num_features = 5000  # Update this based on your actual number of features\n",
        "\n",
        "\n",
        "# Convert the model to ONNX format\n",
        "onnx_model = convert_sparkml(final_model, 'Test',[('features', FloatTensorType([1,num_features]))], spark_session=spark)\n",
        "\n",
        "# Save the ONNX model\n",
        "onnx_path = \"/opt/zeppelin/\"\n",
        "save_model(onnx_model, 'best_model.onnx')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Note converted from Jupyter_2JK4KVJR7"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
